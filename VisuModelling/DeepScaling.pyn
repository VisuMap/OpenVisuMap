# DeepScaling.pyn
#
# Do multidimensional scaling with a neural network. Reduce given table Y to 
# inDim-dimensional table X, a la NN backprobagation: X<-Y
#
#------------------------------------------------------------------------------
import random
vv.Import('CommonUtil.pyn')
vv.Import('VsModelling.pyn')
InitVmd()
vmd.Clear()

#------------------------------------------------------------------------------

xyMap = None
spMap = None
md = None

def Logger(epoch, cost):
	tm = (time.time() - vmd.startTime)/epoch
	UpdateXyMap(X.numpy(), 
		title=f'Epoch: {epoch+1}, Cost: {cost:_.2f}, E/s: {tm:.3f}')

def UpdateSeqView(epoch, cost):
	global spMap
	npSeq = X.numpy().flatten()
	if (spMap == None) or spMap.TheForm.IsDisposed:
		spMap = ShowSpectrum(npSeq)
	else:
		items = spMap.ItemList
		for i in range(items.Count):
			items[i].Value = npSeq[i]
		spMap.ResetScaling()
	return spMap

def UpdateSeqView2(epoch, cost):
	global spMap
	npSeq = X.numpy().flatten()
	if (spMap == None) or spMap.TheForm.IsDisposed:
		spMap = New.MapSnapshot(True).Show()
	minV, maxV = np.min(npSeq), np.max(npSeq)
	rangeV = maxV - minV
	N = npSeq.shape[0]
	pos = np.empty([N, 3], dtype=np.float32)
	posY = 200 if (int(epoch/vmd.reportFreq)%2 == 0) else 400
	sz = spMap.MapLayout.Width
	for i in range(N):
		pos[i] = sz*(npSeq[i] - minV)/rangeV, posY, 0
	spMap.MoveBodiesTo(mm.ToMatrix32(pos), 150, 20)  
	return spMap

def UpdateSeqView3(epoch, cost):
	global spMap
	npSeq = X.numpy().flatten()
	if (spMap == None) or spMap.TheForm.IsDisposed:
		spMap = ShowBarView(npSeq)
	else:
		spMap.MoveTo(mm.ToArray64(npSeq), 20, 30)
	return spMap

preX = None
def UpdateSeqView4(epoch, cost):
	global spMap
	global preX
	if (spMap == None) or spMap.TheForm.IsDisposed:
		spMap = New.HistoryView().Show()
	newX = X.numpy()
	if preX is not None:
		varX = float( np.sum(np.abs(newX-preX)) )
		spMap.AddStep(varX)
	preX = newX
	return spMap

def UpdateOutMap(epoch, cost, title=None):
	pY =  md(X, training=False).numpy()
	outDim = dY.shape[1]
	outMap = Update2DMap(pY) if outDim <= 3 else	UpdateXyMap(pY)
	if title != None:
		outMap.Title = title
	return outMap

def Logger1D(epoch, cost):
	tm = (time.time() - vmd.startTime)/epoch
	logTitle = f'Epoch: {epoch+1}, Cost: {cost:_.2f}, E/s: {tm:.3f}'
	#UpdateSeqView4(epoch, cost).Title = logTitle
	UpdateOutMap(epoch, cost, logTitle)

#-------------------------------------------------------------------------------------------

def DeepScalingModel(X, Y, dimList, DR):
	P = input = keras.Input(shape=(X.shape[1]), dtype=tf.float32)
	for k, dim in enumerate(dimList):
		P = DenseLayer(dim, 'leaky_relu') (P)
		if (k <= 4) and (DR>0): 
			P = keras.layers.Dropout(DR)(P)
		#Jumping connections for better performance
		if k==0: P1 = P  
		if k==4: P += P1
	P = DenseLayer(Y.shape[1], 'sigmoid') (P)
	output = OutScaling(Y, gape=0.0) (P)
	md = keras.Model(input, output, name=f'Scaling')
	md.lossFct = keras.losses.MeanSquaredError()
	md.X = X   # this statment will add X to md.trainable_variables!
	return md

@tf.function(jit_compile=False)
def TrainBatch(md, bIdx):
	with tf.GradientTape() as tape:
		bX, bY = tf.gather(X, bIdx), tf.gather(Y, bIdx)
		pY = md(bX, training=True)
		loss = md.lossFct(bY, pY)
	grads = tape.gradient(loss, md.trainable_variables)
	md.optimizer.apply_gradients(zip(grads, md.trainable_variables))
	return loss

def TrainScalingModel(md, epochs, dsBatchs, LR):
	md.optimizer = AdamOptimizer(epochs*len(dsBatchs), LR)
	vmd.EnableLog()	
	TrainBatch.__init__(TrainBatch.python_function, 'TrainBatch', jit_compile=False)
	vmd.stopTraining = False
	vmd.startTime = time.time()
	logger = Logger1D if inDim == 1 else Logger
	for epoch in range(epochs):  
		cost = 0
		for i, bIdx in enumerate(dsBatchs):
			cost += TrainBatch(md, bIdx)
			if i%50==0: vv.DoEvents()
		random.shuffle(dsBatchs)
		if vmd.stopTraining:	break
		ReportTraining(epoch, cost, logger)
	vmd.trainingTime = time.time() - vmd.startTime

def ToBatchList(dY, inDim, spread, batchSize):
	N = dY.shape[0]
	Y = tf.convert_to_tensor(dY)
	dX = RandomMatrix(spread, N, inDim)
	X = tf.Variable(dX, trainable=True)
	
	#sortKey = [b.Type for b in vv.Dataset.BodyListEnabled()]
	
	keyList = vv.AtlasManager.ReadValueList('Instance1', 'i19')
	if keyList is None or keyList.Count != N:
		vv.Message('Invalid sorting keys')
		vv.Return()
	sortKey = [it.Value for it in keyList]
	
	indexList = np.argsort(sortKey)
	#indexList = np.array(range(N))
	#np.random.shuffle(indexList)
	batchList = []
	for n in range(0, N, batchSize):
		batchList.append( tf.convert_to_tensor(indexList[n:n+batchSize]) )
	return X, Y, dX, batchList

def NewModel(X, Y, dimList, DR, epochs, dsBatchs, LR):
	global md
	md = DeepScalingModel(X, Y, dimList, DR)
	TrainScalingModel(md, epochs, dsBatchs, LR)
	return md

#------------------------------------------------------------------------------
# Load data and settings
inDim, dimList = 3, 6*[256]
spread, batchSize, DR, LR, epochs = 0.0, 25, 0.125, 0.0001, 1000
vmd.reportFreq = 10


dY = GetMapData()
#dY = GetDatasetData()
X, Y, dX, dsBatchs = ToBatchList(dY, inDim, spread, batchSize)

#------------------------------------------------------------------------------
# Create model and train it.
#

md = NewModel(X, Y, dimList, DR, epochs, dsBatchs, LR)
pX, pY = X.numpy(), md(X, training=False).numpy()
meanL1 = np.sum(np.abs(pY-dY))/dY.shape[0]
sTitle = f'Epochs/DR/LR: {epochs}/{DR:.3f}/{LR:.5f}, Mean-L1: {meanL1:.3f}'
ShowMap(pX, title = sTitle)
ShowMap(pY, title = sTitle)

'''
md = NewModel(X, Y, dimList, DR, epochs, dsBatchs, LR)
X0, Y0 = X.numpy(), md(X, training=False).numpy()

Y =  Y - Y0
X.assign(dX)
md = NewModel(X, Y, dimList, DR, epochs, dsBatchs, LR)
X1, Y1 = X.numpy(), md(X, training=False).numpy()

Y =  Y - Y1
X.assign(dX)
md = NewModel(X, Y, dimList, DR, epochs, dsBatchs, LR)
X2, Y2 = X.numpy(), md(X, training=False).numpy()

pX, pY = np.concatenate([X0, X1, X2], axis=1),    Y0+Y1+Y2
meanErr = np.sum(np.abs(pY-dY))/dY.shape[0]
sTitle = f'Epochs/DR/LR: {epochs}/{DR:.3f}/{LR:.5f}, Mean-L1: {meanErr:.3f}'
pX = 5000*pX+np.array([400, 400, 400])
ShowMap(pX, title = sTitle)
ShowMap(pY, title = sTitle)
'''

'''
pX, pY = np.concatenate([X0, X1], axis=1),    Y0+Y1
meanErr = np.sum(np.abs(pY-dY))/dY.shape[0]
sTitle = f'Epochs/DR/LR: {epochs}/{DR:.3f}/{LR:.5f}, Mean-L1: {meanErr:.3f}'
pX = 5000*pX+np.array([400, 400])
ShowMap(pX, title = sTitle)
ShowMap(pY, title = sTitle)
ShowMap(Y0, title = sTitle)
ShowMap(Y1, title = sTitle).NormalizeView()
'''

'''
for n in range(3):
	del md
	md = DeepScalingModel(X, Y, dimList, DR)
	TrainScalingModel(md, epochs, dsBatchs, LR)
	if vmd.stopTraining:	break
	vmd.logMap.NewSnapshot()

for repeats in range(3):
	del X
	del md			
	X = tf.Variable(dX, trainable=True)
	md = DeepScalingModel(X, Y, dimList, DR)
	TrainScalingModel(md, epochs, dsBatchs, LR)
	spMap.DuplicateView()
	vmd.logMap.NewSnapshot()
	if vmd.stopTraining:
		break

UpdateXyMap(X.numpy())
P =  md(X, training=False).numpy()
Update2DMap(dY-P)
Y0.shape

Update2DMap(P)
errMap = np.sum(np.abs(P-dY))/dY.shape[0]
Update2DMap(Y.numpy())
'''
