# DeepScaling.pyn
#
# Do multidimensional scaling with a neural network. Reduce given table Y to 
# inDim-dimensional table X, a la NN backprobagation: X<-Y
#
#------------------------------------------------------------------------------
vv.Import('CommonUtil.pyn')
vv.Import('VsModelling.pyn')
InitVmd()
vmd.Clear()
#------------------------------------------------------------------------------
def RandomMatrix(size, rows, columns):
	if size == 0:
		return np.zeros([rows, columns], dtype=np.float32)
	else:
		return np.random.uniform(-size, size, [rows, columns]).astype(np.float32)

def Logger(epoch, cost):
	global xyMap
	npTable = X.numpy()
	if (xyMap == None) or xyMap.TheForm.IsDisposed:
		xyMap = ShowXyMap(npTable)
	else:
		mm.CopyToTable(npTable, xyMap.GetNumberTable())
		xyMap.Redraw()
	tm = (time.time() - vmd.startTime)/epoch
	xyMap.Title = f'Epoch: {epoch+1}, Cost: {cost:_.2f}, E/s: {tm:.3f}'

def DeepScalingModel(X, Y, dimList, DR):
	inDim = X.shape[1]
	input = keras.Input(shape=(1), dtype=tf.int32)
	P = tf.gather(X, input)
	P = tf.reshape(P, [-1, inDim])	
	for k, dim in enumerate(dimList):
		P = DenseLayer(dim, 'leaky_relu') (P)
		if (k == 0) and (DR>0): 
			P = keras.layers.Dropout(DR)(P)
	P = DenseLayer(Y.shape[1], 'sigmoid') (P)
	output = OutScaling(Y, gape=0.0) (P)
	md = keras.Model(input, output, name=f'Scaling')
	md.lossFct = keras.losses.MeanSquaredError()
	md.X = X   # this statment will add X to md.trainable_variables!
	return md

@tf.function(jit_compile=False)
def TrainBatch(md, bIdx):
	bY = tf.gather(Y, bIdx)
	with tf.GradientTape() as tape:
		loss = md.lossFct(bY, md(bIdx, training=True))
	grads = tape.gradient(loss, md.trainable_variables)
	md.optimizer.apply_gradients(zip(grads, md.trainable_variables))
	return loss

def TrainScalingModel(md, epochs, dsBatchs, LR):
	md.optimizer = AdamOptimizer(epochs*len(dsBatchs), LR)
	vmd.EnableLog()	
	TrainBatch.__init__(TrainBatch.python_function, 'TrainBatch', jit_compile=False)
	vmd.stopTraining = False
	vmd.startTime = time.time()
	for epoch in range(epochs):  
		cost = 0
		for i, bIdx in enumerate(dsBatchs):
			cost += TrainBatch(md, bIdx)
			if i%50==0: vv.DoEvents()
		if vmd.stopTraining:	break
		ReportTraining(epoch, cost, Logger)
	vmd.trainingTime = time.time() - vmd.startTime

#------------------------------------------------------------------------------
# Load data and settings

inDim = 2
batchSize = 25
dimList = [100, 200, 400]
DR, LR, epochs = 0.25, 0.00025, 500
initSpread = 0
vmd.reportFreq = 10
xyMap = None
md = None

#dY = GetMapData()
dY = GetDatasetData()
Y = tf.convert_to_tensor(dY)
N = dY.shape[0]
dX = RandomMatrix(initSpread, N, inDim)
X = tf.Variable(dX, trainable=True)
idxList = np.array(range(N))
dsBatchs = []
for n in range(0, N, batchSize):
	dsBatchs.append( tf.convert_to_tensor(idxList[n:n+batchSize]) )

#------------------------------------------------------------------------------
# Create model and train it.

md = DeepScalingModel(X, Y, dimList, DR)
TrainScalingModel(md, epochs, dsBatchs, LR)

'''

for repeats in range(1):
	X.assign(RandomMatrix(initSpread, N, inDim))
	del md
	xyMap = None
	md = DeepScalingModel(X, Y, dimList, DR)
	TrainScalingModel(md, epochs, dsBatchs, LR)


P = X.numpy()
P =  md(idxList, training=False).numpy()
xy = ShowXyMap(P)
xy.ShowSnapshot()
xy.Close()
dimList.reverse()
'''
