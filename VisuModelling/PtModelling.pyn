#ModellingHelp.pyn
import numpy as np
import math, time
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR
vv.Import('CommonUtil.pyn')

class ModellingConfiguration:
	def __init__(self):
		self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
		self.NN = None
		self.ds = None
		self.log = None
		self.reportFreq = 50
		self.trainingTime = 0

	def EnableLog(self):
		if (self.log == None) or self.log.TheForm.IsDisposed:
		    self.log = New.HistoryView().Show()
		    self.log.OnClose('@vmd.NN.stopTraining=True')

	def Clear(self):
		if self.NN is not None:
		    del self.NN
		    self.NN = None
		if self.ds is not None:
		    del self.ds
		    self.ds = None

class MapDataset(Dataset):
	def __init__(self):
		self.X,  self.Y  = GetDatasetData(), GetMapData()
		self.XX, self.YY = Np2T(self.X), Np2T(self.Y)
		self.Samples = self.X.shape[0]

	def __getitem__(self, index):
		return self.XX[index], self.YY[index]

	def __len__(self):
		return self.Samples

	def GetLoader(self, batch_size=64):
		return DataLoader(dataset=self, batch_size=batch_size, shuffle=True)

class OutScaling(nn.Module):
	def __init__(self, outDim, outData=None):
		super(OutScaling, self).__init__()
		if outData is None:
			colScale = torch.ones([outDim])
			colShift = torch.zeros([outDim])
		else:
			colMax = np.max(outData, axis=0)
			colMin = np.min(outData, axis=0) 
			colCenter = 0.5*(colMax + colMin)
			colFact = 1.025 * (colMax - colMin)
			colScale = torch.from_numpy( colFact )
			colShift = torch.from_numpy( colCenter - 0.5*colFact )
		self.scalings = nn.Parameter(colScale.to(vmd.device), requires_grad=False)
		self.shifts = nn.Parameter(colShift.to(vmd.device), requires_grad=False)

	def forward(self, x):
		return x * self.scalings + self.shifts

class InScaling(nn.Module):
	def __init__(self, inDim, inData=None):
		super(InScaling, self).__init__()
		if inData is None:
			inScale = torch.tensor(1.0)
		else:
			inScale = torch.tensor( 4.0/(np.std(inData) * inData.shape[1]) )			
		self.inScalings = nn.Parameter(inScale.to(vmd.device), requires_grad=False)

	def forward(self, x):
		return x * self.inScalings

class NeuralNetwork(nn.Module):
    def __init__(self, dims, trainData=None, dropRatio=0.25):
        super(NeuralNetwork, self).__init__()
        if trainData is not None:
            dims = [trainData.X.shape[1]] + dims + [trainData.Y.shape[1]]					

        self.model = nn.Sequential()
        inData = None if trainData is None else trainData.Y
        self.model.add_module('inS', InScaling(dims[0], inData))

        L = len(dims)
        for k in range(L-1):
            self.model.add_module('lay%d'%k, nn.Linear(dims[k], dims[k+1], bias=True))
            actFct = nn.Sigmoid() if k==(L-2) else nn.LeakyReLU(negative_slope=0.01)
            self.model.add_module('f%d'%k, actFct)
            if k==0:self.model.add_module('dp', nn.Dropout(p=dropRatio))
        if trainData is None:
            self.model.add_module('os', OutScaling(dims[-1]) )
        else:
            self.model.add_module('os', OutScaling(dims[-1], trainData.Y) )

        self.model.apply(init_weights)
        self.layerDimensions = dims
        self = self.to(vmd.device)
        self.stopTraining = False
        # If the model is intended for training.
        if trainData is not None:  
            vmd.EnableLog()

    def forward(self, x):
        return self.model(x)

    def Train(self, mapDs, epochs, batch_size=25, lr=0.00025, gamma=0.8):	
        batchDs = mapDs.GetLoader(batch_size=batch_size)
        optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        scheduler = StepLR(optimizer, step_size=int(epochs/20), gamma=gamma)
        loss_func = torch.nn.MSELoss(reduction='sum')	
        self.stopTraining = False
        startTime = time.time()
        for epoch in range(epochs):
            cost = 0
            for bX, bY in batchDs:
                loss = loss_func(self(bX), bY)
                optimizer.zero_grad(set_to_none=False)
                loss.backward()
                optimizer.step()
                cost += loss.data.cpu().numpy()
                vv.DoEvents()
            cost /= len(batchDs)
            scheduler.step()
            if self.stopTraining:
                break
            if ((epoch + 1) % vmd.reportFreq == 0):
                ReportTraining(epoch, cost)
        vmd.trainingTime = time.time() - startTime

#========================================================================================================

def CheckGPU():
	vv.Echo( "GPU Available: " + str(torch.cuda.is_available()) )

def InitVmd():
	if 'vmd' not in globals():
		globals()['vmd'] = ModellingConfiguration()

def Np2T(A):
	return torch.from_numpy(A).float().to(vmd.device)

def T2Np(T):
	return T.detach().cpu().numpy()

def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

def ShowPred(NN, XX, refY):
	with torch.no_grad():
		P = T2Np(NN(XX))
	map = ShowMap(P)
	if refY is not None:
		err = np.mean(np.linalg.norm(refY-P, axis=1))
		map.Title = 'Predicated Map  L2 Error: %.2f, Time: %.1fs'%(err, vmd.trainingTime)
	else:
		map.Title = 'Predicated Map'
	map.Show()


