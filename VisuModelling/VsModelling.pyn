import sys, math, time
import numpy as np
import tensorflow as tf
from tensorflow import keras

class ModellingConfiguration:
	def __init__(self):
		self.model = None
		self.ds = None
		self.log = None
		self.logMap = None
		self.reportFreq = 10
		self.trainingTime = 0		
		self.REGRESSION = 100
		self.CLUSTERING = 101
		self.FULL_MODEL = 102

	def EnableLog(self):
		if (self.log == None) or self.log.TheForm.IsDisposed:
			self.log = New.HistoryView().Show()
			self.log.OnClose('@vmd.stopTraining=True')

	def Clear(self):
		vv.Map.SelectedBodies=None
		if self.model is not None:
			del self.model.lossFct
			del self.model
			self.model = None
		if self.ds is not None:
			if self.ds.tensors is not None:
				del self.ds.tensors
				self.ds.tensors = None
			del self.ds
			self.ds = None

	def OpenLogMap(self):
		if (self.logMap == None) or self.logMap.TheForm.IsDisposed:
				self.logMap = New.MapSnapshot(True).Show()

class OutScaling(keras.layers.Layer):
	def __init__(self, Y, gape=0.0125):
		super(OutScaling, self).__init__()
		colMin, colMax = np.min(Y, axis=0), np.max(Y, axis=0) 
		self.colScale = (colMax - colMin)/(1-2*gape)
		self.colShift = 0.5*(colMax + colMin - self.colScale)
	def call(self, Y):
		return self.colScale * Y + self.colShift

class InScaling(keras.layers.Layer):
	def __init__(self, X):
		super(InScaling, self).__init__()
		self.inScale = float(4.0/(np.std(X) * X.shape[1]))
	def call(self, X):
		return self.inScale * X

class FullMapLoss(tf.keras.losses.Loss):
	def __init__(self, mapDim):
		super(FullMapLoss, self).__init__()
		self.mapDim = mapDim
		self.MSE = keras.losses.MeanSquaredError()
		self.Entropy = keras.losses.CategoricalCrossentropy()
	def __call__(self, Y, pY):
		mD = self.mapDim
		return self.MSE(Y[:,:mD], pY[:,:mD]) + self.Entropy(Y[:, mD:], pY[:, mD:])

class TrainDataset():
	def __init__(self, X, Y, batchSize=25, shuffle=True, sortKey=None):
		if X.shape[0] != Y.shape[0]:
			vv.Message(f'Input and Output data have different size: {X.shape[0]} != {Y.shape[0]}')
			vv.Return()
		if sortKey is not None:
			idxList = np.argsort(sortKey)
			sX, sY = X[idxList], Y[idxList]
		else:
			sX, sY = X, Y					
		if shuffle:
			idxList = np.arange(X.shape[0])
			np.random.shuffle(idxList)
			sX = np.take(sX, idxList, axis=0)
			sY = np.take(sY, idxList, axis=0)
		self.tensors = []
		for p in range(0, X.shape[0], batchSize):
			q = p+batchSize
			self.tensors.append(( tf.convert_to_tensor(sX[p:q]), tf.convert_to_tensor(sY[p:q]) ))
		self.X, self.Y = X, Y

	def __iter__(self):
		self.idxTensor = 0
		return self
	def __next__(self):
		if self.idxTensor < len(self.tensors):
			result = self.tensors[self.idxTensor]
			self.idxTensor += 1
			return result
		else:
			raise StopIteration
#-------------------------------------------------------------------------

def InitVmd():
	if 'vmd' not in globals():
		globals()['vmd'] = ModellingConfiguration()

def CheckGPU():
	vv.Echo(f'GPU Available: {tf.test.is_gpu_available()}' )
	vv.Echo(f'Num GPUs Available: {len(tf.config.list_physical_devices("GPU"))}')
	vv.Echo(f'Device Name: {tf.test.gpu_device_name()}')

def GetClusterPred(md, X):
	return tf.math.argmax( md(X, training=False), axis=1).numpy()

def ShowClusterPred(T, epoch, cost, map):
	pBodyList = map.BodyList
	for i, b in enumerate(pBodyList):
		b.Type = T[i]
	map.RedrawBodiesType()
	mismatchs = 0
	bList = vv.Dataset.BodyListEnabled()
	for i in range(bList.Count):
		if bList[i].Type != pBodyList[i].Type:
			mismatchs += 1
			pBodyList[i].Highlighted = True
		else:
			pBodyList[i].Highlighted = False
	if cost == 0:
		map.Title = f'Clustering mismatchs: {mismatchs}'
	else:
		map.Title = f'Epoch: {epoch+1}, Cost: {cost:.2f}, Mismatchs: {mismatchs}'

def ShowModel(md):
	md.summary(print_fn = lambda x: vv.Echo(x))

def GetHits(mapDim, P, T):
	N = P.shape[0]
	hits = np.zeros(N, dtype=np.int8)
	idxList = tf.math.argmax(P[:, mapDim:], axis=1).numpy() + mapDim
	for k in range(N):
		if T[k, idxList[k]] == 1.0:
			hits[k] = 1
	return hits

def ShowPred(md, X, refY):
	if md.modelType == vmd.REGRESSION:
		P = vmd.model(X, training=False).numpy()
		map = ShowMap(P)
		if refY is not None:
			err = np.mean(np.linalg.norm(refY-P, axis=1))
			map.Title = f'Predicated Map:  L2 Error: {err:.2f}, Time: {vmd.trainingTime:.1f}'
		else:
			map.Title = 'Predicated Map'
	elif md.modelType == vmd.CLUSTERING:
		map = New.MapSnapshot(True).Show()
		P = GetClusterPred(md, X)
		ShowClusterPred(P, 0, 0, map)
	else:  # vmd.FULL_MODEL
		P = md(X, training=False).numpy()
		mD = md.mapDim
		Pm = P[:, :mD].copy()
		Pc = np.argmax(P[:, mD:], axis=1)
		map = ShowMap(Pm)
		ShowClusterPred(Pc, 0, 0, map)
		if refY is not None:
			misses = P.shape[0] - np.sum( GetHits(mD, P, refY) )
			err = np.mean(np.linalg.norm(refY[:,:mD]-Pm, axis=1))
			map.Title = f'Predicated Map:  L2 Error: {err:.2f}, MisMatchs: {misses}, Time: {vmd.trainingTime:.1f}'
		else:
			map.Title = 'Predicated Map'

'''
ShowPred(vmd.model, X, Y)
'''

def DenseLayer(dim, activation):
	return keras.layers.Dense(dim, activation=activation, use_bias=True,
			kernel_initializer='glorot_uniform', bias_initializer='zeros')

#-------------------------------------------------------------------------------------

@tf.function(jit_compile=True)
def train_step(md, optimizer, lossFct, bX, bY):
	with tf.GradientTape() as tape:
		loss = lossFct(bY, md(bX, training=True))
	grads = tape.gradient(loss, md.trainable_variables)
	optimizer.apply_gradients(zip(grads, md.trainable_variables))
	return loss

def TrainModel(vmd, epochs, initial_lr=0.00025, logCallback=None):
	epochBatches = len(vmd.ds.tensors)
	totalBatches = epochs * epochBatches
	lr_schedule = keras.optimizers.schedules.ExponentialDecay(
		initial_learning_rate=initial_lr,
		decay_steps=int(totalBatches/20), 
		staircase=False, decay_rate=0.8)
	optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)

	#The following line makes train_step() callable for different dataset.
	train_step.__init__(train_step.python_function, 'train_step', jit_compile=True)

	vmd.EnableLog()
	vmd.stopTraining = False
	startTime = time.time()
	for epoch in range(epochs):
		cost = 0
		for bX, bY in vmd.ds:
			cost += train_step(vmd.model, optimizer, vmd.model.lossFct, bX, bY)
		vv.DoEvents()
		cost /= epochBatches
		if vmd.stopTraining:
			break
		if ((epoch + 1) % vmd.reportFreq == 0):
			ReportTraining(epoch, cost)
			if logCallback != None:
				logCallback(epoch, cost)
	vmd.trainingTime = time.time() - startTime
