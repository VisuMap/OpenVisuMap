import sys, math, time
import numpy as np
import tensorflow as tf
from tensorflow import keras

class ModellingConfiguration:
	def __init__(self):
		self.model = None
		self.ds = None
		self.log = None
		self.logMap = None
		self.reportFreq = 10
		self.trainingTime = 0
		self.REGRESSION = 100
		self.CLUSTERING = 101

	def EnableLog(self):
		if (self.log == None) or self.log.TheForm.IsDisposed:
			self.log = New.HistoryView().Show()
			self.log.OnClose('@vmd.stopTraining=True')

	def Clear(self):
		vv.Map.SelectedBodies=None
		if self.model is not None:
			del self.model
			self.model = None
		if self.ds is not None:
			if self.ds.tensors is not None:
				del self.ds.tensors
				self.ds.tensors = None
			del self.ds
			self.ds = None

class OutScaling(keras.layers.Layer):
	def __init__(self, X, gape=0.0125):
		super(OutScaling, self).__init__()
		colMin, colMax = np.min(X, axis=0), np.max(X, axis=0) 
		self.colScale = (colMax - colMin)/(1-2*gape)
		self.colShift = 0.5*(colMax + colMin - self.colScale)
	def call(self, X):
		return self.colScale * X + self.colShift

class InScaling(keras.layers.Layer):
	def __init__(self, X):
		super(InScaling, self).__init__()
		self.inScale = float(4.0/(np.std(X) * X.shape[1]))
	def call(self, X):
		return self.inScale * X

class RBFLayer(keras.layers.Layer):
	def __init__(self, X, L):
		super(RBFLayer, self).__init__()
		minV, maxV = np.min(X, axis=0), np.max(X, axis=0), 
		cs = np.empty([L*L, 2])
		ss = 1.0/L
		x0, y0 = 0.5*ss, 0.5*ss
		for p in range(L):
			for q in range(L):
				cs[p*L+q] = [x0+p*ss, y0+q*ss]
		cs = 0.9*cs + [0.05,0.05]
		sigma = np.max(maxV - minV)/(3.0*L)
		self.betas = 0.5/sigma**2
		self.centers = cs.astype(np.float32) * (maxV - minV) + minV

	def call(self, X):
		# the following block is from 
		#   https://github.com/PetraVidnerova/rbf_for_tf2/blob/master/rbflayer.py
		H = tf.transpose(tf.expand_dims(self.centers, -1) - tf.transpose(X))
		return tf.exp(-self.betas * tf.math.reduce_sum(H**2, axis=1))

#-------------------------------------------------------------------------

def InitVmd():
	if 'vmd' not in globals():
		globals()['vmd'] = ModellingConfiguration()

def CheckGPU():
	vv.Echo(f'GPU Available: {tf.test.is_gpu_available()}' )
	vv.Echo(f'Num GPUs Available: {len(tf.config.list_physical_devices("GPU"))}')
	vv.Echo(f'Device Name: {tf.test.gpu_device_name()}')

def MapLogger(epoch, cost):
	if (vmd.logMap == None) or vmd.logMap.TheForm.IsDisposed:
			vmd.logMap = New.MapSnapshot(True).Show()
	Y = vmd.model(vmd.ds.X, training=False).numpy()
	vmd.logMap.MoveBodiesTo(mm.ToMatrix32(Y))
	err = np.mean(np.linalg.norm(vmd.ds.Y-Y, axis=1))
	vmd.logMap.Title = f'Epoch: {epoch+1}, Cost: {cost:.2f},  L2: {err:.2f}'

def GetClusterPred(md, X):
	return tf.math.argmax( md(X, training=False), axis=1).numpy()

def ShowClusterPred(T, epoch, cost, map):
	pBodyList = map.BodyList
	for i, b in enumerate(pBodyList):
		b.Type = T[i]
	map.RedrawBodiesType()
	mismatchs = 0
	bList = vv.Dataset.BodyListEnabled()
	for i in range(bList.Count):
		if bList[i].Type != pBodyList[i].Type:
			mismatchs += 1
			pBodyList[i].Highlighted = True
		else:
			pBodyList[i].Highlighted = False
	if cost == 0:
		map.Title = f'Clustering mismatchs: {mismatchs}'
	else:
		map.Title = f'Epoch: {epoch+1}, Cost: {cost:.2f}, Mismatchs: {mismatchs}'

def ClusterLogger(epoch, cost):
	if (vmd.logMap == None) or vmd.logMap.TheForm.IsDisposed:
		vmd.logMap = New.MapSnapshot(True).Show()
	P = GetClusterPred(vmd.model, vmd.ds.X)
	ShowClusterPred(P, epoch, cost, vmd.logMap)

def ShuffleDataset(X, Y):
	idxList = np.arange(X.shape[0])
	np.random.shuffle(idxList)
	X = np.take(X, idxList, axis=0)
	Y = np.take(Y, idxList, axis=0)

def TrainDataset(X, Y, batchSize=25):
		if X.shape[0] != Y.shape[0]:
			vv.Message(f'Input and Output data have different size: {X.shape[0]} != {Y.shape[0]}')
			vv.Return()
		ds = tf.data.Dataset.from_tensor_slices((X, Y))
		ds = ds.shuffle(buffer_size=1024).batch(batch_size=batchSize)
		ds.tensors = []
		for bX, bY in ds:
			ds.tensors.append( (tf.convert_to_tensor(bX), tf.convert_to_tensor(bY)) )
		ds.X = X
		ds.Y = Y
		return ds

def ShowModel(md):
	md.summary(print_fn = lambda x: vv.Echo(x))

def ShowPred(md, X, refY):
	if md.modelType == vmd.REGRESSION:
		P = vmd.model(X, training=False).numpy()
		map = ShowMap(P)
		if refY is not None:
			err = np.mean(np.linalg.norm(refY-P, axis=1))
			map.Title = f'Predicated Map:  L2 Error: {err:.2f}, Time: {vmd.trainingTime:.1f}'
		else:
			map.Title = 'Predicated Map'
	else:
		map = New.MapSnapshot(True).Show()
		P = GetClusterPred(md, X)
		ShowClusterPred(P, 0, 0, map)

#-------------------------------------------------------------------------------------

def DenseLayer(dim, activation):
	return keras.layers.Dense(dim, activation=activation, use_bias=True,
			kernel_initializer='glorot_uniform', bias_initializer='zeros')

def NewMapModel(layerDims, X, Y, dropoutRatio=0.25):
	md = keras.Sequential()
	md.add( keras.Input(shape=(X.shape[1])) )
	md.add( InScaling(X) )
	for k, dim in enumerate(layerDims):
		md.add( DenseLayer(dim, 'leaky_relu') )
		if (k == 0) and (dropoutRatio>0): 
			md.add(keras.layers.Dropout(dropoutRatio))
	md.add( DenseLayer(Y.shape[1], 'sigmoid') )
	md.add( OutScaling(Y, gape=0.0) )
	md.modelType = vmd.REGRESSION
	return md

def NewTransModel(layerDims, X, Y, dropoutRatio=0.25, rbfDim=256):
	md = keras.Sequential()
	md.add( keras.Input(shape=(X.shape[1])) )
	md.add( RBFLayer(X, rbfDim) )
	for k, dim in enumerate(layerDims):
		md.add( DenseLayer(dim, 'leaky_relu') )
		if (k == 0) and (dropoutRatio>0): 
			md.add(keras.layers.Dropout(dropoutRatio))
	md.add( DenseLayer(Y.shape[1], 'sigmoid') )
	md.add( OutScaling(Y, gape=0.0) )
	md.modelType = vmd.REGRESSION
	return md


def NewCluster(layerDims, X, Y, dropoutRatio=0.25):
	md = keras.Sequential()
	md.add( keras.Input(shape=(X.shape[1])) )
	md.add( InScaling(X) )
	for k, dim in enumerate(layerDims):
		md.add( DenseLayer(dim, 'leaky_relu') )
		if (k == 0) and (dropoutRatio>0): 
			md.add(keras.layers.Dropout(dropoutRatio))
	md.add( DenseLayer(Y.shape[1], 'leaky_relu') )
	md.add( keras.layers.Softmax(name='ClstLayer') )
	md.modelType = vmd.CLUSTERING
	return md

#-------------------------------------------------------------------------------------

@tf.function(jit_compile=True)
def train_step(md, optimizer, lossFct, bX, bY):
	with tf.GradientTape() as tape:
		loss = lossFct(bY, md(bX, training=True))
	grads = tape.gradient(loss, md.trainable_variables)
	optimizer.apply_gradients(zip(grads, md.trainable_variables))
	return loss

def TrainModel(vmd, epochs, initial_lr=0.00025, logCallback=None):
	if vmd.model.modelType == vmd.REGRESSION:
		lossFct = keras.losses.MeanSquaredError()
	else:
		lossFct = keras.losses.CategoricalCrossentropy()

	epochBatches = len(list(vmd.ds))
	totalBatches = epochs * epochBatches
	lr_schedule = keras.optimizers.schedules.ExponentialDecay(
		initial_learning_rate=initial_lr,
		decay_steps=int(totalBatches/20), 
		staircase=False, decay_rate=0.8)
	optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)

	#The following line makes train_step() callable for different dataset.
	train_step.__init__(train_step.python_function, 'train_step', jit_compile=True)

	vmd.EnableLog()
	vmd.stopTraining = False
	startTime = time.time()
	for epoch in range(epochs):
		cost = 0
		for bX, bY in vmd.ds.tensors:
			cost += train_step(vmd.model, optimizer, lossFct, bX, bY)
		vv.DoEvents()
		cost /= epochBatches
		if vmd.stopTraining:
			break
		if ((epoch + 1) % vmd.reportFreq == 0):
			ReportTraining(epoch, cost)
			if logCallback != None:
				logCallback(epoch, cost)
	vmd.trainingTime = time.time() - startTime
