import sys, math, time
import numpy as np
import tensorflow as tf
from tensorflow import keras
vv.Import('CommonUtil.pyn')

class ModellingConfiguration:
	def __init__(self):
		self.model = None
		self.ds = None
		self.log = None
		self.logMap = None
		self.reportFreq = 50
		self.trainingTime = 0
	def EnableLog(self):
		if (self.log == None) or self.log.TheForm.IsDisposed:
			self.log = New.HistoryView().Show()
			self.log.OnClose('@vmd.stopTraining=True')
	def Clear(self):
		if self.model is not None:
			del self.model
			self.model = None
		if self.ds is not None:
			if self.ds.tensors is not None:
				del self.ds.tensors
				self.ds.tensors = None
			del self.ds
			self.ds = None

class OutScaling(keras.layers.Layer):
	def __init__(self, D, gape=0.0125):
		super(OutScaling, self).__init__()
		colMax = np.max(D, axis=0)
		colMin = np.min(D, axis=0) 
		colCenter = 0.5*(colMax + colMin)
		colFact = (1 + 2*gape)*(colMax - colMin)
		self.colScale = colFact
		self.colShift =  colCenter - 0.5*colFact
	def call(self, inputs):
		return self.colScale * inputs + self.colShift

class InScaling(keras.layers.Layer):
	def __init__(self, D):
		super(InScaling, self).__init__()
		maxInput = np.max(D)
		minInput = np.min(D)
		range = maxInput - minInput
		fct = 1.0
		self.inScale = fct/range
		self.inShift = -fct*minInput/range
	def call(self, inputs):
		#return (inputs - minInput)/range
		return self.inScale * inputs + self.inShift

#-------------------------------------------------------------------------
def InitVmd():
	if 'vmd' not in globals():
		globals()['vmd'] = ModellingConfiguration()

def CheckGPU():
	vv.Echo( "GPU Available: " + str( tf.test.is_gpu_available() ))
	vv.Echo('Num GPUs Available: %d'%(len(tf.config.list_physical_devices('GPU'))))
	vv.Echo('Device Name: ' + tf.test.gpu_device_name())

def MapLogger(epoch, cost):
	if (vmd.logMap == None) or vmd.logMap.TheForm.IsDisposed:
			vmd.logMap = New.MapSnapshot(True).Show()
	Y = vmd.model(vmd.ds.X, training=False).numpy()
	ShowMap(Y, vmd.logMap)
	vmd.logMap.Title = f'Epoch: {epoch+1}, Cost: {cost:.2f}'

def ShuffleDataset(X, Y):
	idxList = np.arange(X.shape[0])
	np.random.shuffle(idxList)
	X = np.take(X, idxList, axis=0)
	Y = np.take(Y, idxList, axis=0)

def TrainDataset(X, Y, batchSize=25):
		if X.shape[0] != Y.shape[0]:
			vv.Message("Input and Output data have different size: %d != %d"%(X.shape[0], Y.shape[0]))
			vv.Return()
		ds = tf.data.Dataset.from_tensor_slices((X, Y))
		ds = ds.shuffle(buffer_size=1024).batch(batch_size=batchSize)
		ds.tensors = []
		for bX, bY in ds:
			ds.tensors.append( (tf.convert_to_tensor(bX), tf.convert_to_tensor(bY)) )
		ds.X = X
		ds.Y = Y
		return ds

def ShowModel(md):
	md.summary(print_fn = lambda x: vv.Echo(x))

def ShowPred(P, refY):
	map = ShowMap(P)
	if refY is not None:
		err = np.mean(np.linalg.norm(refY-P, axis=1))
		map.Title = 'Predicated Map:  L2 Error: %.2f, Time: %.1fs'%(err, vmd.trainingTime)
	else:
		map.Title = 'Predicated Map'

#-------------------------------------------------------------------------------------

def NewMapModel(layerDims, X, Y, dropoutRatio=0.25):
	md = keras.Sequential()
	inDim, outDim = X.shape[1], Y.shape[1]
	md.add( keras.Input(shape=(inDim)) )
	lastLayer = len(layerDims) - 1
	md.add(InScaling(X))
	
	for k, dim in enumerate(layerDims):
		md.add( keras.layers.Dense(dim, activation='leaky_relu', use_bias=True,
			kernel_initializer='glorot_uniform', bias_initializer='zeros' ) )
		if (k == 0) and (dropoutRatio>0): 
			md.add(keras.layers.Dropout(dropoutRatio))

	md.add( keras.layers.Dense(outDim, activation='sigmoid', use_bias=True,
		kernel_initializer='glorot_uniform', bias_initializer='zeros' ) )
	md.add(OutScaling(Y, gape=0.015))

	return md

#-------------------------------------------------------------------------------------

@tf.function(jit_compile=True)
def train_step(md, optimizer, lossFct, bX, bY):
	with tf.GradientTape() as tape:
		loss = lossFct(md(bX, training=True), bY)
	grads = tape.gradient(loss, md.trainable_variables)
	optimizer.apply_gradients(zip(grads, md.trainable_variables))
	return loss

def TrainModel(vmd, epochs=200, initial_lr=0.00025, logCallback=None):
	lossFct = keras.losses.MeanSquaredError()
	epochBatches = len(list(vmd.ds))
	totalBatches = epochs * epochBatches
	lr_schedule = keras.optimizers.schedules.ExponentialDecay(
		initial_learning_rate=initial_lr,
		decay_steps=int(totalBatches/20), 
		staircase=True, decay_rate=0.8)
	optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)


	#The following line makes train_step() callable for different dataset.
	train_step.__init__(train_step.python_function, 'train_step', jit_compile=True)
	vmd.EnableLog()
	vmd.stopTraining = False
	startTime = time.time()
	for epoch in range(epochs):
		cost = 0
		for bX, bY in vmd.ds.tensors:
			cost += train_step(vmd.model, optimizer, lossFct, bX, bY)
		vv.DoEvents()
		cost /= epochBatches
		if vmd.stopTraining:
			break
		if ((epoch + 1) % vmd.reportFreq == 0):
			ReportTraining(epoch, cost)
			if logCallback != None:
				logCallback(epoch, cost)
	vmd.trainingTime = time.time() - startTime
